Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job stats:
job                      count
---------------------  -------
run_downloads_DB_load        1
total                        1

Select jobs to execute...

[Wed Jan  3 20:51:04 2024]
rule run_downloads_DB_load:
    jobid: 0
    reason: Rules with neither input nor output files are always executed.
    resources: tmpdir=/var/folders/yn/wljgxb2x5x58r3d6fg3hspf00000gp/T

[Wed Jan  3 20:51:05 2024]
Error in rule run_downloads_DB_load:
    jobid: 0

RuleException:
CalledProcessError in file /Users/AwotoroE-Dev/Desktop/DBMerge/Snakefile, line 3:
Command 'set -euo pipefail;  bash /Users/AwotoroE-Dev/Desktop/DBMerge/.snakemake/scripts/tmpbrogyixm.DBFile.sh' returned non-zero exit status 1.
  File "/Users/AwotoroE-Dev/Desktop/DBMerge/Snakefile", line 3, in __rule_run_downloads_DB_load
  File "/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-01-03T205104.933976.snakemake.log

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data from Hosts Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.api_utils import ilove_access, cochrane_access, medline_class_access, ovid_new_access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cochrane_service = cochrane_access()\n",
    "print(cochrane_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilove_access()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medline_class_access(searchText=[\"\"\"\n",
    "(\n",
    "  (review[pt] OR \"review, tutorial\"[pt] OR \"review, academic\"[pt])\n",
    "  AND \n",
    "  (\n",
    "    medline[tw] OR medlars[tw] OR embase[tw] OR pubmed[tw] OR cochrane[tw]\n",
    "    OR scisearch[tw] OR psychinfo[tw] OR psycinfo[tw]\n",
    "    OR psychlit[tw] OR psyclit[tw] \n",
    "    OR cinahl[tw] \n",
    "    OR ((hand[tw] AND search*[tw]) OR (manual*[tw] AND search*[tw]))\n",
    "    OR (\"electronic database*\"[tw] OR \"bibliographic database*\"[tw] OR \"computerized database*\"[tw] OR \"online database*\"[tw])\n",
    "    OR pooling[tw] OR pooled[tw] OR \"mantel haenszel\"[tw]\n",
    "    OR peto[tw] OR dersimonian[tw] OR \"der simonian\"[tw] OR \"fixed effect\"[tw]\n",
    "    OR \"retraction of publication\"[pt] OR \"retracted publication\"[pt]\n",
    "  )\n",
    ")\n",
    "OR\n",
    "(\n",
    "  meta-analysis[pt] \n",
    "  OR meta-analysis[sh] \n",
    "  OR (meta-analys*[tw] OR meta analys*[tw] OR metaanalys*[tw])\n",
    "  OR (systematic*[tw] AND review*[tw])\n",
    "  OR (quantitative*[tw] AND review*[tw])\n",
    "  OR (methodologic*[tw] AND review*[tw])\n",
    "  OR (\"integrative research review\"[tw] OR \"research integration\"[tw])\n",
    ")\n",
    "AND\n",
    "(\n",
    "  immunization[mesh] \n",
    "  OR Immunization Programs[mesh] \n",
    "  OR vaccines[mesh]\n",
    "  OR (immunisation[tiab] OR immunization[tiab] OR immunise[tiab] OR immunize[tiab] OR vaccine[tiab])\n",
    ")\n",
    "AND humans[filter]\n",
    "AND \n",
    "(\"2011\"[edat] : \"3000\"[edat])  # Retrieves records from 2011 till date\n",
    "\"\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovid_new_access()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Tagging Process is Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Commands.PaperProcessorPipeline import PaperProcessorPipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def main():\n",
    "    pipeline = PaperProcessorPipeline(\n",
    "        table_name='all_db',\n",
    "        column_mapping={'Id': 'primary_id'}\n",
    "    )\n",
    "\n",
    "    sources = [\n",
    "        # {\n",
    "        #     \"query\": \"SELECT primary_id, \\\"DOI\\\", doi_url, \\\"Source\\\" FROM all_db WHERE \\\"Source\\\"='Cochrane'\",\n",
    "        #     \"csv_file_path\": \"Data/output/papers_data\",\n",
    "        #     \"db_name\": \"Cochrane\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"query\": \"SELECT primary_id, \\\"DOI\\\", doi_url, \\\"Source\\\" FROM all_db WHERE \\\"Source\\\"='LOVE'\",\n",
    "        #     \"csv_file_path\": \"Data/output/papers_data_love\",\n",
    "        #     \"db_name\": \"LOVE\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"query\": \"SELECT primary_id, \\\"DOI\\\", doi_url, \\\"Source\\\" FROM all_db WHERE \\\"Source\\\"='OVID'\",\n",
    "        #     \"csv_file_path\": \"Data/output/papers_data_OVID\",\n",
    "        #     \"db_name\": \"OVID\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"query\": \"SELECT primary_id, \\\"DOI\\\", doi_url, \\\"Source\\\" FROM all_db WHERE \\\"Source\\\"='Medline'\",\n",
    "        #     \"csv_file_path\": \"Data/output/papers_data_medline\",\n",
    "        #     \"db_name\": \"Medline\"\n",
    "        # },\n",
    "        {\n",
    "            \"query\": \"SELECT primary_id, \\\"DOI\\\", \\\"Source\\\" FROM all_db WHERE \\\"DOI\\\" IS NOT NULL AND \\\"DOI\\\" != '' AND primary_id=1\",\n",
    "            \"csv_file_path\": \"Data/output/papers_data_all\",\n",
    "            \"db_name\": \"all\"\n",
    "        }\n",
    "    ]\n",
    "    #  AND primary_id=1\n",
    "    # Use parallel processing to process multiple sources simultaneously\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(\n",
    "            lambda source: pipeline.process_source_in_batches(\n",
    "                query=source[\"query\"],\n",
    "                csv_file_path=source[\"csv_file_path\"],\n",
    "                db_name=source[\"db_name\"],\n",
    "                batch_size=10\n",
    "            ),\n",
    "            sources\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import docx  # For Word documents\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def save_to_file(file_path, data):\n",
    "    \"\"\"\n",
    "    Save the given data to a file.\n",
    "\n",
    "    :param file_path: The file path where the data will be saved.\n",
    "    :param data: The data to save (e.g., a list or string).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        if isinstance(data, list):\n",
    "            file.write('\\n'.join(data))  # Write each item in a new line\n",
    "        else:\n",
    "            file.write(data)  # Write directly if it's a string\n",
    "\n",
    "\n",
    "from src.Utils.Helpers import get_contents\n",
    "\n",
    "\n",
    "# Example 1: Extract text from a URL (HTML)\n",
    "url = \"https://journals.sagepub.com/doi/10.1177/2150131917742299\"\n",
    "# url = \"https://www.sciencedirect.com/science/article/abs/pii/S0091743514003260?via=ihub\"\n",
    "# url = \"https://www.tandfonline.com/doi/full/10.1080/01443615.2022.2162867\"\n",
    "result = get_contents(url)\n",
    "save_to_file(\"new_extracted_text.html\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to test some basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import altair as alt\n",
    "import ast\n",
    "from src.Commands.DatabaseUpdater import DatabaseUpdater\n",
    "\n",
    "# updater = DatabaseUpdater(table_name=\"all_db\", column_mapping={'Id': 'Id'})\n",
    "\n",
    "def convert_chart(chart):\n",
    "    try:\n",
    "        # Get the currently active data transformer\n",
    "        current_transformer = alt.data_transformers.get()\n",
    "        # Check if the current transformer is vegafusion\n",
    "        if 'vegafusion' in str(current_transformer):\n",
    "            return chart.to_dict(format=\"vega\")\n",
    "        else:\n",
    "            return chart.to_dict()\n",
    "    except ValueError as e:\n",
    "        print(f\"Handling ValueError: {e}\")\n",
    "        return chart.to_dict()\n",
    "    \n",
    "def view_trends_by_database_year():\n",
    "    from src.Services.PostgresService import PostgresService, QueryHelper\n",
    "    db_service = PostgresService()\n",
    "    query = f'SELECT * FROM all_db'\n",
    "    # Execute the query using the db_service\n",
    "    record = db_service.execute_raw_query(query)\n",
    "    final_data = record\n",
    "    if \"database\" in final_data:\n",
    "        # Rename databases for better readability\n",
    "        databases = {\n",
    "            'OVID': 'OVID',\n",
    "            'LOVE': 'LOVE DB',\n",
    "            'Medline': 'Medline',\n",
    "            'Cochrane': 'Cochrane'\n",
    "        }\n",
    "        final_data['Journal'] = final_data['Journal'].replace(databases)\n",
    "    return final_data\n",
    "\n",
    "def plot_stacked_bar_chart_altair(data):\n",
    "    \"\"\"\n",
    "    Reads a JSON file, processes the data, and plots a stacked bar chart \n",
    "    with Year on the x-axis and Journals as colors using Altair.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge 'Publication_Year' and 'Year' columns to ensure all rows have a year value\n",
    "    for entry in data:\n",
    "        entry[\"Year\"] = entry.get(\"Publication_Year\") or entry.get(\"Year\")  # Use Publication_Year first if available\n",
    "\n",
    "    # Extract relevant fields: Year, Journal, and study counts from study_types field\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        year = entry.get(\"Year\", \"Unknown\")\n",
    "        journal = entry.get(\"Journal\", \"Unknown\")\n",
    "        study_types = entry.get(\"study_types\", \"{}\")\n",
    "\n",
    "        # Convert study_types from string to dict\n",
    "        try:\n",
    "            study_types_dict = json.loads(study_types) if isinstance(study_types, str) else study_types\n",
    "            for study_type, count in study_types_dict.items():\n",
    "                records.append({\"Year\": year, \"Journal\": journal, \"Study Type\": study_type, \"Count\": count})\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Convert Year to numeric, ensuring sorting works correctly\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "\n",
    "    # Generate a stacked bar chart in Altair\n",
    "    chart = alt.Chart(df).mark_bar().encode(\n",
    "        x=alt.X(\"Year:O\", title=\"Year\", sort=\"ascending\"),\n",
    "        y=alt.Y(\"sum(Count)\", title=\"Number of Studies\"),\n",
    "        color=alt.Color(\"Journal:N\", title=\"Journal\"),\n",
    "        tooltip=[\"Year\", \"Journal\", \"sum(Count)\"]\n",
    "    ).properties(\n",
    "        title=\"Stacked Bar Chart of Study Types by Year (Colored by Journal)\",\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    # Display chart\n",
    "    return chart\n",
    "\n",
    "# Example usage\n",
    "data = view_trends_by_database_year()\n",
    "plot_stacked_bar_chart_altair(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Tagging Process with Rough DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Commands.PaperProcessorPipeline import PaperProcessorPipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def main():\n",
    "    pipeline = PaperProcessorPipeline(\n",
    "        table_name='rough_db',\n",
    "        column_mapping={'Id': 'primary_id'}\n",
    "    )\n",
    "\n",
    "    sources = [\n",
    "        {\n",
    "            \"query\": \"SELECT primary_id, \\\"Link to full text\\\" FROM rough_db WHERE primary_id > 0 and \\\"Link to full text\\\" != ''\",\n",
    "            \"csv_file_path\": \"output/papers_data_rough\",\n",
    "            \"db_name\": \"ROUGH\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Use parallel processing to process multiple sources simultaneously\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(\n",
    "            lambda source: pipeline.process_source_in_batches(\n",
    "                query=source[\"query\"],\n",
    "                csv_file_path=source[\"csv_file_path\"],\n",
    "                db_name=source[\"db_name\"],\n",
    "                batch_size=10\n",
    "            ),\n",
    "            sources\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import date2num\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the Gantt chart data\n",
    "data = {\n",
    "    \"Task\": [\n",
    "        # Year 1 - Foundation and Methodology Development\n",
    "        \"Initial Literature Review\",\n",
    "        \"Data Exploration and Initial AMR Analysis\",\n",
    "        \"Data Quality & Initial Preprocessing\",\n",
    "        \"Prototype AI/ML and Visualization Techniques\",\n",
    "        \"Publication 1 Draft (Methodology & AMR Trends Overview)\",\n",
    "        \"Publication 1 Submission & Feedback\",\n",
    "\n",
    "        # Year 2 - Advanced Analysis of Imputation Methods\n",
    "        \"Advanced Data Preprocessing & Feature Engineering\",\n",
    "        \"Imputation Method Development and Evaluation\",\n",
    "        \"Publication 2 Draft (Comparative Imputation Methods)\",\n",
    "        \"Publication 2 Submission & Feedback\",\n",
    "\n",
    "\n",
    "        # Year 3 - Final Analysis and Manuscript Preparation\n",
    "        \"Co-resistance and Pattern Analysis\",\n",
    "        \"Visualization and Network Analysis\",\n",
    "        \"Model Validation and Feedback Integration\",\n",
    "        \"Final Analysis,Interpretation and Synthesis of Findings\",\n",
    "        \"Publication 3 Draft (Final Model, Findings & Conclusion)\",\n",
    "         \"Publication 3 Submission & Feedback\",\n",
    "        \"Dissertation Preparation\",\n",
    "        \"Dissertation Submission\",\n",
    "        \"Final Defense Preparation\",\n",
    "        \"Final Defense and Presentation\"\n",
    "    ],\n",
    "    \"Start\": [\n",
    "        # Year 1\n",
    "        \"2024-10-01\", \"2024-10-15\", \"2024-11-15\", \"2025-01-01\", \"2025-03-01\", \"2025-05-01\",\n",
    "\n",
    "        # Year 2\n",
    "        \"2025-07-01\", \"2025-08-15\", \"2025-10-01\", \"2026-01-01\", \"2026-03-01\",\n",
    "\n",
    "        # Year 3\n",
    "         \"2026-07-01\", \"2026-08-15\",\"2026-10-01\", \"2026-11-01\", \"2027-01-01\",\n",
    "       \"2027-05-01\",\"2027-07-01\",\"2027-08-01\"\n",
    "\n",
    "    ],\n",
    "    \"End\": [\n",
    "        # Year 1\n",
    "        \"2024-10-14\",\"2024-12-31\", \"2024-12-31\", \"2025-02-28\", \"2025-04-30\", \"2025-06-30\",\n",
    "\n",
    "        # Year 2\n",
    "        \"2025-08-14\", \"2025-09-30\",  \"2025-12-31\",  \"2026-02-28\", \"2026-06-30\",\n",
    "\n",
    "        # Year 3\n",
    "          \"2026-08-14\",  \"2026-09-30\", \"2026-10-31\", \"2026-12-31\", \"2027-04-30\",\n",
    "         \"2027-06-30\", \"2027-07-31\", \"2027-12-31\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert Start and End to datetime\n",
    "df[\"Start\"] = pd.to_datetime(df[\"Start\"])\n",
    "df[\"End\"] = pd.to_datetime(df[\"End\"])\n",
    "\n",
    "# Get the current date for the vertical line\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Create the Gantt chart\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "for i, task in df.iterrows():\n",
    "    color = \"skyblue\"  # All tasks are considered \"In Progress\" initially\n",
    "    ax.barh(task[\"Task\"], date2num(task[\"End\"]) - date2num(task[\"Start\"]),\n",
    "            left=date2num(task[\"Start\"]), color=color, edgecolor=\"black\")\n",
    "\n",
    "# Add the current date line\n",
    "ax.axvline(date2num(current_date), color=\"red\", linestyle=\"--\", linewidth=2, label=\"Today\")\n",
    "\n",
    "# Add labels and formatting\n",
    "ax.set_title(\"3-Year PhD Timeline (Quarterly Breakdown)\", fontsize=16)\n",
    "ax.set_xlabel(\"Timeline (Quarters)\", fontsize=12)\n",
    "ax.set_ylabel(\"Tasks\", fontsize=12)\n",
    "ax.xaxis_date()\n",
    "ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "ax.invert_yaxis()  # Flip tasks to start from the top\n",
    "\n",
    "# Add custom legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], color=\"skyblue\", lw=6, label=\"In Progress\"),\n",
    "    plt.Line2D([0], [0], color=\"red\", lw=2, linestyle=\"--\", label=\"Today\")\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"upper right\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Services.Factories.GeneralPDFScraper.OVIDPDFWebScraper import OVIDPDFWebScraper\n",
    "from src.Services.Factories.GeneralPDFScraper.CochranePDFWebScraper import CochranePDFWebScraper\n",
    "from src.Services.Factories.GeneralPDFScraper.LOVEPDFWebScraper import LOVEPDFWebScraper\n",
    "from src.Commands.TaggingSystem import Tagging\n",
    "\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "def flatten_tags(tags):\n",
    "    \"\"\"Convert nested lists or other complex data types to flattened strings.\"\"\"\n",
    "    for key, value in tags.items():\n",
    "        if isinstance(value, list):\n",
    "            # Select only the last list if multiple lists exist\n",
    "            if isinstance(value[-1], list):\n",
    "                value = value[-1]  # Take the last list for processing\n",
    "            \n",
    "            # Convert the selected list to a comma-separated string\n",
    "            tags[key] = \", \".join(map(str, value)) if isinstance(value, list) else str(value)\n",
    "\n",
    "        elif isinstance(value, str):\n",
    "            tags[key] = value.strip()  # Remove unnecessary whitespace\n",
    "        \n",
    "    return tags\n",
    "\n",
    "gen = LOVEPDFWebScraper(DB_name=\"LOVE\")\n",
    "document = gen.set_doi_url(\n",
    "    # \"https://dx.doi.org/10.1002/14651858.CD013479\"\n",
    "    # \"https://journals.sagepub.com/doi/10.1177/2150131917742299\"\n",
    "    # \"https://www.scielo.br/j/rpp/a/3MSXkXzft7QTJg3ZXg8RPbq/?lang=en\"\n",
    "    # \"https://dx.doi.org/10.1002/14651858.CD013479\"\n",
    "    # \"https://journal.waocp.org/article_88640.html\"\n",
    "    \"https://bmjopen.bmj.com/content/11/12/e052682\"\n",
    "    # \"https://www.sciencedirect.com/science/article/pii/S0264410X22004406?via%3Dihub\"\n",
    "    # \"https://www.tandfonline.com/doi/full/10.1080/21645515.2016.1201623#d1e223\"\n",
    ").fetch_and_extract_first_valid_pdf_text()\n",
    "\n",
    "tag = Tagging(document)\n",
    "from src.Commands.regexp import searchRegEx\n",
    "data = tag.create_columns_from_text(searchRegEx)\n",
    "data = flatten_tags(data)\n",
    "# ff = pd.DataFrame(data)\n",
    "# ff.to_csv(\"testing.csv\")\n",
    "data\n",
    "tag.sections.available_sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_sagepub_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://google.com\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "url = \"https://journals.sagepub.com/doi/10.1177/2150131917742299\"\n",
    "content = fetch_sagepub_content(url)\n",
    "print(content[:1000])  # Print first 1000 characters for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Commands.TaggingSystem import Tagging\n",
    "from src.Commands.regexp import searchRegEx\n",
    "document = \"\"\"\n",
    "===== SearchStrategy =====\n",
    "In this systematic review and meta-analysis, we searched PubMed and Embase from Jan 1, 2004, to March 31, 2015. Test-negative design studies of influenza VE were eligible if they enrolled outpatients on the basis of predefined illness criteria, reported subtype-level VE by season, used PCR to confirm influenza, and adjusted for age. We excluded studies restricted to hospitalised patients or special populations, duplicate reports, interim reports superseded by a final report, studies of live-attenuated vaccine, and studies of prepandemic seasonal vaccine against H1N1pdm09. Two reviewers independently assessed titles and abstracts to identify articles for full review. Discrepancies in inclusion and exclusion criteria and VE estimates were adjudicated by consensus. Outcomes were VE against H3N2, H1N1pdm09, H1N1 (pre-2009), and type B. We calculated pooled VE using a random-effects model.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Assuming self.document is already set\n",
    "tagging = Tagging(document)\n",
    "result = tagging.extract_last_literature_search_dates()\n",
    "# result = tagging.create_columns_from_text(searchRegEx)\n",
    "print(result)\n",
    "# 18, 20, 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_month_year(text):\n",
    "    pattern = r\"\\b(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s*(?:\\d{1,2}(?:st|nd|rd|th)?)?,?\\s*(\\d{4})\\b\"\n",
    "    \n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)} {match.group(2)}\"\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"We conducted a systematic search in three electronic databases from inception up to 13th of January, 2020, without language restrictions.\"\"\"\n",
    "\n",
    "result = extract_month_year(text)\n",
    "print(result)  # Output: January 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_study_types(text, terms_list):\n",
    "    \"\"\"\n",
    "    Extract study types and their counts from text.\n",
    "    Returns a dictionary with study types and counts.\n",
    "    \"\"\"\n",
    "    # Extract study type terms from the terms_list\n",
    "    study_terms = [item for item, _ in terms_list]\n",
    "    \n",
    "    # Regex to match study types and their counts\n",
    "    study_pattern = re.compile(\n",
    "        rf\"(\\d+)\\s*({'|'.join(re.escape(term) for term in study_terms)})\", \n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    matches = study_pattern.findall(text)\n",
    "    \n",
    "    study_types = {}\n",
    "    for count, study_type in matches:\n",
    "        study_type_lower = study_type.lower()\n",
    "        if study_type_lower in study_types:\n",
    "            study_types[study_type_lower] += int(count)\n",
    "        else:\n",
    "            study_types[study_type_lower] = int(count)\n",
    "    \n",
    "    return study_types\n",
    "\n",
    "# Example usage\n",
    "terms_list = [\n",
    "    (\"study\", \"sty\"), (\"studies\", \"sty\"),\n",
    "    (\"RCT\", \"rct\"),\n",
    "    ('randomized controlled trial', \"rct\"),\n",
    "    ('randomised controlled trial', \"rct\"),\n",
    "    ('randomized trial', \"rct\"),\n",
    "    ('randomised trial', \"rct\"),\n",
    "    ('clinical trial', \"rct\"),\n",
    "    (\"double-blind study\", \"rct\"), \n",
    "    (\"placebo-controlled\", \"rct\"),\n",
    "    (\"randomised comparative\", \"rct\"),\n",
    "    (\"NRSI\", \"nrsi\"), \n",
    "    (\"non-randomized studies of interventions\", \"nrsi\"),\n",
    "    (\"observational studies\", \"nrsi\"), \n",
    "    (\"quasi-experimental\", \"nrsi\"), \n",
    "    (\"non-randomized controlled study\", \"nrsi\"), \n",
    "    (\"natural experiment\", \"nrsi\"),\n",
    "    (\"test-negative designs\", \"nrsi\"),\n",
    "    (\"cross-sectional study\", \"nrsi\"), \n",
    "    (\"controlled clinical\", \"nrsi\"), \n",
    "    (\"cohort study\", \"nrsi\"), \n",
    "    (\"prospective study\", \"nrsi\"), \n",
    "    (\"retrospective study\", \"nrsi\"), \n",
    "    (\"longitudinal study\", \"nrsi\"),\n",
    "    (\"case-control study\", \"nrsi\"),\n",
    "    (\"pre-post studies\", \"nrsi\"),\n",
    "    (\"interrupted time series\", \"nrsi\"),\n",
    "    (\"case reports\", \"nrsi\"),\n",
    "    (\"case series\", \"nrsi\"),\n",
    "    (\"mixed methods\", \"mmtd\"),\n",
    "    (\"convergent design\", \"mmtd\"), \n",
    "    (\"explanatory sequential design\", \"mmtd\"),\n",
    "    (\"qualitative study\", \"quanti\"),\n",
    "]\n",
    "\n",
    "text = \"\"\"\n",
    "20 in Germany, 10 in Nigeria, Ghana(5), five in spain. 2 quantitative study\n",
    "10 cross sectional, 2 quanitative\n",
    "\"\"\"\n",
    "\n",
    "print(extract_study_types(text, terms_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sex_distribution(text):\n",
    "    \"\"\"\n",
    "    Extract sex distribution (male, female, other) from text.\n",
    "    Returns a dictionary with percentages and values.\n",
    "    \"\"\"\n",
    "    # Regex to match percentages and values for male, female, and other genders\n",
    "    sex_pattern = re.compile(\n",
    "        r\"(\\d{1,3}%)\\s*(male|female|divers|other)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    matches = sex_pattern.findall(text)\n",
    "    \n",
    "    sex_distribution = {}\n",
    "    for percentage, gender in matches:\n",
    "        sex_distribution[gender.lower()] = percentage\n",
    "    \n",
    "    return sex_distribution\n",
    "\n",
    "def extract_study_types(text):\n",
    "    \"\"\"\n",
    "    Extract study types and their counts from text.\n",
    "    Returns a dictionary with study types and counts.\n",
    "    \"\"\"\n",
    "    # Regex to match study types and their counts\n",
    "    study_pattern = re.compile(\n",
    "        r\"(\\d+)\\s*(cross\\s*sectional|mixed\\s*method|qualitative|quantitative|longitudinal|case\\s*study)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    matches = study_pattern.findall(text)\n",
    "    \n",
    "    study_types = {}\n",
    "    for count, study_type in matches:\n",
    "        study_types[study_type.lower()] = int(count)\n",
    "    \n",
    "    return study_types\n",
    "\n",
    "def extract_total_population(text):\n",
    "    \"\"\"\n",
    "    Extract the total population size (N) from text.\n",
    "    Returns the population size as an integer.\n",
    "    \"\"\"\n",
    "    # Regex to match population size (N)\n",
    "    population_pattern = re.compile(\n",
    "        r\"N\\s*[-—]?\\s*population\\s*[:]?\\s*(\\d{1,}(?:\\s*\\d{3})*)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    match = population_pattern.search(text)\n",
    "    if match:\n",
    "        # Remove spaces and commas from the number\n",
    "        population = match.group(1).replace(\" \", \"\").replace(\",\", \"\")\n",
    "        return int(population)\n",
    "    return None\n",
    "\n",
    "def extract_study_characteristics(text):\n",
    "    \"\"\"\n",
    "    Extract characteristics of included studies:\n",
    "    - Study types and counts\n",
    "    - Countries and counts\n",
    "    - Sample size and sex distribution\n",
    "    - Population health status\n",
    "    Returns a dictionary with all extracted information.\n",
    "    \"\"\"\n",
    "    characteristics = {}\n",
    "    \n",
    "    # Extract study types\n",
    "    characteristics[\"study_types\"] = extract_study_types(text)\n",
    "    \n",
    "    # Extract countries\n",
    "    country_pattern = re.compile(\n",
    "        r\"(\\b[A-Z][a-z]+(?: [A-Z][a-z]+)*\\b)\\s*\\((\\d+)\\)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    countries = country_pattern.findall(text)\n",
    "    characteristics[\"countries\"] = {country: int(count) for country, count in countries}\n",
    "    \n",
    "    # Extract sample size and sex distribution\n",
    "    sample_pattern = re.compile(\n",
    "        r\"Sample\\s*size:\\s*(\\d+)\\s*\\((.*?)\\)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    sample_match = sample_pattern.search(text)\n",
    "    if sample_match:\n",
    "        characteristics[\"sample_size\"] = int(sample_match.group(1))\n",
    "        characteristics[\"sex_distribution\"] = extract_sex_distribution(sample_match.group(2))\n",
    "    \n",
    "    # Extract population health status\n",
    "    health_status_pattern = re.compile(\n",
    "        r\"Population\\s*health\\s*status:\\s*(.*)\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    health_status_match = health_status_pattern.search(text)\n",
    "    if health_status_match:\n",
    "        characteristics[\"health_status\"] = health_status_match.group(1).strip()\n",
    "    \n",
    "    return characteristics\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "49% male, 48% female, 3% divers.\n",
    "In another study, 99% female, 1% other.\n",
    "\"\"\"\n",
    "print(extract_sex_distribution(text))\n",
    "text = \"\"\"\n",
    "Study type of included studies: 6 cross sectional, 2 mixed method, 2 qualitative.\n",
    "Another study: 10 cross sectional, 2 qualitative.\n",
    "\"\"\"\n",
    "print(extract_study_types(text))\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "N - population 411 300.\n",
    "Another study: N = 1,000,000.\n",
    "\"\"\"\n",
    "print(extract_total_population(text))\n",
    "\n",
    "text = \"\"\"\n",
    "Characteristics of included studies in review:\n",
    "• 10 cross sectional, 2 qualitative\n",
    "• Germany (6), Spain (3), England (2), France (1)\n",
    "• Sample size: 300 (99% female; 1% other)\n",
    "• Population health status: Pregnant women\n",
    "\"\"\"\n",
    "print(extract_study_characteristics(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_inclusion_exclusion_counts(text):\n",
    "    \"\"\"\n",
    "    Uses precompiled regex patterns to extract specific study counts.\n",
    "    Returns a dictionary with the keys: yielded, selected, excluded, and inclusion.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        # Matches numbers after \"yielded\" (e.g., \"yielded 42\")\n",
    "        \"yielded\": re.compile(r\"yielded\\s+(\\d+)\", re.IGNORECASE),\n",
    "        \n",
    "        # Matches numbers after \"selected\" or \"selected for\" (e.g., \"selected 142 for full review\")\n",
    "        \"selected\": re.compile(r\"selected\\s+(\\d+)\\s+(?:for|as|relevant|full\\s+review)\", re.IGNORECASE),\n",
    "        \n",
    "        # Matches numbers after \"excluded\" or \"were excluded\" (e.g., \"262 were excluded\")\n",
    "        \"excluded\": re.compile(r\"(?:excluded|were excluded)\\s+(\\d+)\", re.IGNORECASE),\n",
    "        \n",
    "        # Matches numbers before \"met the inclusion criteria\" (e.g., \"42 met the inclusion criteria\")\n",
    "        \"inclusion\": re.compile(r\"(\\d+)\\s+met\\s+the\\s+inclusion\\s+criteria\", re.IGNORECASE),\n",
    "        \n",
    "        # Matches numbers after \"of which\" (e.g., \"of which 42 met the inclusion criteria\")\n",
    "        \"inclusion_of_which\": re.compile(r\"of\\s+which\\s+(\\d+)\\s+met\\s+the\\s+inclusion\\s+criteria\", re.IGNORECASE),\n",
    "        \n",
    "        # Matches numbers after \"included\" (e.g., \"included 56 in the meta-analysis\")\n",
    "        \"included\": re.compile(r\"included\\s+(\\d+)\\s+(?:in|for)\", re.IGNORECASE),\n",
    "        \n",
    "        # Matches numbers after \"identified\" (e.g., \"identified 305 studies\")\n",
    "        \"identified\": re.compile(r\"identified\\s+(\\d+)\\s+studies\", re.IGNORECASE),\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        \"yielded\": None,\n",
    "        \"selected\": None,\n",
    "        \"excluded\": None,\n",
    "        \"inclusion\": None,\n",
    "        \"included\": None,\n",
    "        \"identified\": None,\n",
    "    }\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            results[key] = int(match.group(1))\n",
    "    \n",
    "    # If \"inclusion\" is not found, try \"inclusion_of_which\"\n",
    "    if results[\"inclusion\"] is None:\n",
    "        match = patterns[\"inclusion_of_which\"].search(text)\n",
    "        if match:\n",
    "            results[\"inclusion\"] = int(match.group(1))\n",
    "    \n",
    "    # If \"inclusion\" is still not found, try \"included\"\n",
    "    if results[\"inclusion\"] is None:\n",
    "        match = patterns[\"included\"].search(text)\n",
    "        if match:\n",
    "            results[\"inclusion\"] = int(match.group(1))\n",
    "    \n",
    "    return json.dumps(results, indent=4)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Of the full-texts, 42 (11%) met inclusion criteria yielding data from 5 RCTs and 39 observational studies over 23 influenza seasons (from 1983/84 up to the mid-season of 2016/17). \n",
    "We identified 3368 unduplicated publications, selected 142 for full review, and included 56 in the meta-analysis. \n",
    "After removing duplicates, we identified 2592 potential studies. Following title and abstract screening, 305 studies were identified for full text review. Of these, 262 were excluded, leaving a total of 45 studies, of which 37 studies used the TND.\n",
    "\"\"\"\n",
    "\n",
    "print(extract_inclusion_exclusion_counts(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Commands.TaggingSystem import Tagging\n",
    "from src.Commands.regexp import searchRegEx\n",
    "from src.Services.Factories.Sections.ArticleExtractorFactory import ArticleExtractorFactory\n",
    "\n",
    "def extract_all_sections_from_text(text):\n",
    "    \"\"\"\n",
    "    Extracts all sections and their respective content from a string variable.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing sections and their content.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are section headers and values are their respective content.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    current_section = \"Title\"  # Default section for content before any section header\n",
    "    current_content = []\n",
    "\n",
    "    # Split the text into lines for processing\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Check if the line is a potential section header\n",
    "        if stripped_line.isupper() or stripped_line.endswith(\":\"):\n",
    "            # Save the previous section and its content\n",
    "            if current_section:\n",
    "                sections[current_section] = \"\\n\".join(current_content)\n",
    "\n",
    "            # Start a new section\n",
    "            current_section = stripped_line.rstrip(\":\")\n",
    "            current_content = []\n",
    "        else:\n",
    "            # Add content to the current section\n",
    "            if stripped_line:  # Skip empty lines\n",
    "                current_content.append(stripped_line)\n",
    "\n",
    "    # Add the last section's content\n",
    "    if current_section:\n",
    "        sections[current_section] = \"\\n\".join(current_content)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def save_to_file(file_path, data):\n",
    "    \"\"\"\n",
    "    Save the given data to a file.\n",
    "\n",
    "    :param file_path: The file path where the data will be saved.\n",
    "    :param data: The data to save (e.g., a list or string).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        if isinstance(data, list):\n",
    "            file.write('\\n'.join(data))  # Write each item in a new line\n",
    "        else:\n",
    "            file.write(data)  # Write directly if it's a string\n",
    "            \n",
    "# Assuming self.document is already set\n",
    "# url_tandfonline = 'https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-018-1098-3'\n",
    "# url_tandfonline = 'https://www.tandfonline.com/doi/full/10.1080/01443615.2022.2162867'\n",
    "# url_tandfonline = 'https://pmc.ncbi.nlm.nih.gov/articles/PMC8021610/'\n",
    "url_tandfonline = 'https://pmc.ncbi.nlm.nih.gov/articles/PMC8477621/'\n",
    "# url_tandfonline = \"https://www.mdpi.com/1660-4601/19/15/9425\"\n",
    "# url_tandfonline = \"https://www.nature.com/articles/s41598-021-83727-7\"\n",
    "# url_tandfonline = \"https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-020-08753-y\"\n",
    "# url_tandfonline = \"https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-018-1098-3\"\n",
    "# url_tandfonline = \"https://www.jpmh.org/index.php/jpmh/article/view/998\"\n",
    "# url_tandfonline = \"https://obgyn.onlinelibrary.wiley.com/doi/10.1111/aogs.14359\"\n",
    "# url_tandfonline = \"https://www.aerzteblatt.de/int/archive/article/161392\"\n",
    "# url_tandfonline = \"https://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-14-867\"\n",
    "# url_tandfonline = \"https://www.scielo.br/j/rpp/a/3MSXkXzft7QTJg3ZXg8RPbq/?lang=en\"\n",
    "# url_tandfonline = \"https://journal.waocp.org/article_88640.html\"\n",
    "# for PDF\n",
    "# url_tandfonline = \"https://ijpsr.com/wp-content/uploads/2015/03/57-Vol.-6-Issue-4-April-2015-IJPSR-RA-5231-Paper-57.pdf\"\n",
    "# url_tandfonline = \"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD013717.pub2/pdf/full\"\n",
    "# url_tandfonline = \"https://journals.sagepub.com/doi/10.1177/2150131917742299\"\n",
    "# url_tandfonline = \"https://bmjopen.bmj.com/content/8/4/e019206\"\n",
    "url_tandfonline = \"https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD013479/full\"\n",
    "tandfonline_extractor = ArticleExtractorFactory.get_extractor(url=url_tandfonline)\n",
    "print(\"Available sections (tandfonline):\", tandfonline_extractor.get_available_sections())\n",
    "document = tandfonline_extractor.get_section(\"Main Content\")\n",
    "\n",
    "file_path = 'extracted_dates.txt'\n",
    "save_to_file(file_path, document)\n",
    "\n",
    "tagging = Tagging(document)\n",
    "result = tagging.create_columns_from_text(searchRegEx)\n",
    "# print(result)\n",
    "from itertools import chain\n",
    "def flatten_tags(tags):\n",
    "    \"\"\"Convert nested lists or other complex data types to flattened strings.\"\"\"\n",
    "    for key, value in tags.items():\n",
    "        if isinstance(value, list):\n",
    "            # Select only the last list if multiple lists exist\n",
    "            if isinstance(value[-1], list):\n",
    "                value = value[-1]  # Take the last list for processing\n",
    "            \n",
    "            # Convert the selected list to a comma-separated string\n",
    "            tags[key] = \", \".join(map(str, value)) if isinstance(value, list) else str(value)\n",
    "\n",
    "        elif isinstance(value, str):\n",
    "            tags[key] = value.strip()  # Remove unnecessary whitespace\n",
    "        \n",
    "    return tags\n",
    "print(result)\n",
    "print(flatten_tags(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tandfonline_extractor.get_section(\"SearchStrategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_search_strategy(html_content):\n",
    "    \"\"\"\n",
    "    Extracts the Search Strategy section from an HTML document.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): The HTML content of the document.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text from the Search Strategy section.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Find all sections with potential \"Search Strategy\" headings\n",
    "    search_patterns = [\n",
    "        r\"search\\s*strategy\",\n",
    "        r\"literature\\s*search\",\n",
    "        r\"search\\s*methods?\",\n",
    "        r\"search\\s*terms?\",\n",
    "        r\"database\\s*search\",\n",
    "        r\"electronic\\s*databases?\",\n",
    "        r\"search\\s*methodology\",\n",
    "        r\"search\\s*and\\s*selection\\s*process\"\n",
    "    ]\n",
    "\n",
    "    for section in soup.find_all(\"section\"):\n",
    "        heading = section.find([\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "        if heading and any(re.search(pattern, heading.text, re.IGNORECASE) for pattern in search_patterns):\n",
    "            return section.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return \"Search Strategy section not found.\"\n",
    "\n",
    "# Example Usage:\n",
    "html_data = \"\"\"\n",
    "<section id=\"sec-2-1\"><h3>Search Strategy</h3><div role=\"paragraph\">\n",
    "A comprehensive literature search was conducted using five electronic databases related to health care, namely, \n",
    "the PubMed, CINAHL, Cochrane Library, Medline, and PsycInfo. Searches were limited to articles published between \n",
    "January 2006 and March 4, 2017, as the HPV vaccine has only been licensed since 2006. A search was conducted using \n",
    "the keywords: adolescen* OR girl* OR boy* OR male OR female OR parent*; AND human papillomavirus vaccine* OR HPV; \n",
    "AND uptake; AND knowledge* OR barrier* OR accept* OR intent*. Reference lists of review articles were retrieved \n",
    "to identify additional sources of literature.\n",
    "</div></section>\n",
    "\"\"\"\n",
    "\n",
    "# Extract the search strategy section\n",
    "result = extract_search_strategy(html_data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Text Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Services.Factories.Sections.LiteratureSearchQAML import LiteratureSearchQA\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample text from a journal article (you can update this with your own content)\n",
    "    text = \"\"\"\n",
    "    ===== Main Content =====\n",
    "    ===== Title =====\n",
    "    Determinants of influenza vaccine hesitancy among pregnant women in Europe: a systematic review - PMC\n",
    "\n",
    "    ===== Abstract =====\n",
    "    Background Pregnant women are at high risk for severe influenza. However, maternal influenza vaccination uptake in most World Health Organization (WHO) European Region countries remains low, despite the presence of widespread national recommendations. An influenza vaccination reduces influenza-associated morbidity and mortality in pregnancy, as well as providing newborns with protection in their first months. Potential determinants of vaccine hesitancy need to be identified to develop strategies that can increase vaccine acceptance and uptake among pregnant women. The primary objective of the systematic review is to identify the individual determinants of influenza vaccine hesitancy among pregnant women in Europe, and how to overcome the hesitancy.\n",
    "    Methods Databases were searched for peer-reviewed qualitative and quantitative studies published between 2009 and 2019 inclusive. Databases included PubMed via MEDLINE, Cochrane Central Register for Controlled Trials, PsycINFO, SAGE Journals, Taylor and Francis and Springer nature. These covered themes including psychology, medicine, and public health. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach, 11 studies were eligible and analyzed for significant determinants of influenza vaccine hesitancy among pregnant women in Europe.\n",
    "    Results The most commonly reported factors were psychological aspects, for example concerns about safety and risks to mother and child, or general low risk perception of becoming ill from influenza. Doubts about the effectiveness of the vaccine and a lack of knowledge about this topic were further factors. There was also influence of contextual factors, such as healthcare workers not providing adequate knowledge about the influenza vaccine or the pregnant lady stating their antivaccine sentiment.\n",
    "    Conclusion Health promotion that specifically increases knowledge among pregnant women about influenza and vaccination is important, supporting a valid risk judgment by the pregnant lady. The development of new information strategies for dialogue between healthcare providers and pregnant women should form part of this strategy.\n",
    "    Keywords: Influenza, Vaccination, Infectious diseases, Pregnant women, Europe, Vaccine hesitancy, Vaccine refusal, Vaccine delay, Review, Maternal\n",
    "\n",
    "    ===== Paper Type =====\n",
    "    open access\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Instantiate the class\n",
    "    qa_model = LiteratureSearchQA()\n",
    "    \n",
    "    # Extract information\n",
    "    last_search_date = qa_model.extract_last_search_date(text)\n",
    "    total_studies = qa_model.extract_total_studies_included(text)\n",
    "    total_population = qa_model.extract_total_population(text)\n",
    "    total_sample_size = qa_model.extract_total_sample_size(text)\n",
    "    sex_proportion = qa_model.extract_sex_proportion(text)\n",
    "    rct_count = qa_model.extract_total_RCT_count(text)\n",
    "    nsri_count = qa_model.extract_total_NSRI_count(text)\n",
    "    mix=qa_model.extract_total_mix_method_count(text)\n",
    "    country = qa_model.extract_country_proportion(text)\n",
    "    popu = qa_model.extract_population_proportion(text)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Last literature search date: {last_search_date}\")\n",
    "    print(f\"Total studies included: {total_studies}\")\n",
    "    print(f\"Total population: {total_population}\")\n",
    "    print(f\"Total sample size: {total_sample_size}\")\n",
    "    print(f\"Sex proportion distribution: {sex_proportion}\")\n",
    "    print(f\"RCT count: {rct_count}\")\n",
    "    print(f\"NSRI count: {nsri_count}\")\n",
    "    print(f\"Mix count: {mix}\")\n",
    "    print(f\"country count: {country}\")\n",
    "    print(f\"popu count: {popu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Utils.Helpers import html_to_plain_text_selenium\n",
    "from src.Commands.TaggingSystem import Tagging\n",
    "from src.Commands.regexp import searchRegEx\n",
    "from src.Services.Factories.Sections.ArticleExtractorFactory import ArticleExtractorFactory\n",
    "\n",
    "def extract_all_sections_from_text(text):\n",
    "    \"\"\"\n",
    "    Extracts all sections and their respective content from a string variable.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing sections and their content.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are section headers and values are their respective content.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    current_section = \"Title\"  # Default section for content before any section header\n",
    "    current_content = []\n",
    "\n",
    "    # Split the text into lines for processing\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Check if the line is a potential section header\n",
    "        if stripped_line.isupper() or stripped_line.endswith(\":\"):\n",
    "            # Save the previous section and its content\n",
    "            if current_section:\n",
    "                sections[current_section] = \"\\n\".join(current_content)\n",
    "\n",
    "            # Start a new section\n",
    "            current_section = stripped_line.rstrip(\":\")\n",
    "            current_content = []\n",
    "        else:\n",
    "            # Add content to the current section\n",
    "            if stripped_line:  # Skip empty lines\n",
    "                current_content.append(stripped_line)\n",
    "\n",
    "    # Add the last section's content\n",
    "    if current_section:\n",
    "        sections[current_section] = \"\\n\".join(current_content)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def save_to_file(file_path, data):\n",
    "    \"\"\"\n",
    "    Save the given data to a file.\n",
    "\n",
    "    :param file_path: The file path where the data will be saved.\n",
    "    :param data: The data to save (e.g., a list or string).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        if isinstance(data, list):\n",
    "            file.write('\\n'.join(data))  # Write each item in a new line\n",
    "        else:\n",
    "            file.write(data)  # Write directly if it's a string\n",
    "            \n",
    "url_tandfonline = \"https://journals.sagepub.com/doi/10.1177/2150131917742299\"\n",
    "soup = html_to_plain_text_selenium(\n",
    "    url_tandfonline, \n",
    "    headless=False\n",
    ")\n",
    "\n",
    "tandfonline_extractor = ArticleExtractorFactory.get_extractor(soup=soup, url=url_tandfonline)\n",
    "print(\"Available sections (tandfonline):\", tandfonline_extractor.get_available_sections())\n",
    "document = tandfonline_extractor.get_section(\"Main Content\")\n",
    "\n",
    "file_path = 'extracted_dates.txt'\n",
    "save_to_file(file_path, document)\n",
    "\n",
    "tagging = Tagging(document)\n",
    "result = tagging.create_columns_from_text(searchRegEx)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Configure Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Avoid detection\n",
    "\n",
    "# Set up WebDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://journals.sagepub.com/doi/10.1177/2150131917742299\"\n",
    "\n",
    "# Open page and get content\n",
    "driver.get(url)\n",
    "html_content = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "print(html_content)  # Print first 1000 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from src.Services.Factories.Sections.SectionExtractor import SectionExtractor\n",
    "\n",
    "# Example Usage\n",
    "with open(\"./running_away.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    document_text = file.read()\n",
    "\n",
    "extract_sections = SectionExtractor(document_text)\n",
    "\n",
    "# Retrieve specific sections\n",
    "abstract_content = extract_sections.get(\"Introduction\")\n",
    "methods_content = extract_sections.get(\"Methods\")\n",
    "extract_sections.get(\"main content\")\n",
    "extract_sections.available_sections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cochrane Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Services.Factories.Sections.ArticleExtractorFactory import ArticleExtractorFactory\n",
    "from bs4 import BeautifulSoup\n",
    "with open(\"file_path.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "\n",
    "extractor = ArticleExtractorFactory.get_extractor(soup=html_soup, url=\"cochranelibrary.com\")\n",
    "print(\"Available sections (tandfonline):\", extractor.get_available_sections())\n",
    "extractor._extract_sections()\n",
    "# extractor.save_extracted_sections(output_path)\n",
    "# print(f\"Extracted sections saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Services.Factories.Sections.ArticleExtractorFactory import ArticleExtractorFactory\n",
    "\n",
    "# Usage Example\n",
    "file_path = \"./testing.txt\"  # Input file\n",
    "output_path = \"./extracted_sections.txt\"  # Output file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_content = file.read()\n",
    "            \n",
    "extractor = ArticleExtractorFactory.get_extractor(text_content=text_content)\n",
    "print(\"Available sections (tandfonline):\", extractor.get_available_sections())\n",
    "extractor._extract_sections()\n",
    "extractor.save_extracted_sections(output_path)\n",
    "print(f\"Extracted sections saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_sections(file_path):\n",
    "    \"\"\"\n",
    "    Extracts all sections and their respective content from a text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are section headers and values are their respective content.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    current_section = \"General\"  # Default section for content before any section header\n",
    "    current_content = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "\n",
    "            # Check if the line is a potential section header\n",
    "            if stripped_line.isupper() or stripped_line.endswith(\":\"):\n",
    "                # Save the previous section and its content\n",
    "                if current_section:\n",
    "                    sections[current_section] = \"\\n\".join(current_content)\n",
    "\n",
    "                # Start a new section\n",
    "                current_section = stripped_line.rstrip(\":\")\n",
    "                current_content = []\n",
    "            else:\n",
    "                # Add content to the current section\n",
    "                if stripped_line:  # Skip empty lines\n",
    "                    current_content.append(stripped_line)\n",
    "\n",
    "        # Add the last section's content\n",
    "        if current_section:\n",
    "            sections[current_section] = \"\\n\".join(current_content)\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Usage example:\n",
    "file_path = \"./extracted_dates.txt\"\n",
    "sections_with_content = extract_sections(file_path)\n",
    "\n",
    "# Print all sections and their respective content\n",
    "sections_with_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_last_literature_search_dates(document):\n",
    "    \"\"\"\n",
    "    Extracts literature search dates from the given text.\n",
    "    :param document: str, the text to extract dates from.\n",
    "    :return: list, sorted unique dates extracted.\n",
    "    \"\"\"\n",
    "    if not document or not isinstance(document, str):\n",
    "        raise ValueError(\"The document content is empty or invalid. Please provide a valid string.\")\n",
    "\n",
    "    # Use the merged regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        (?:(?:searched\\s+from\\s+inception\\s+to|date\\s+of\\s+last\\s+literature\\s+search|last\\s+search\\s+date|\n",
    "        the\\s+search\\s+was\\s+conducted|all\\s+searches\\s+were\\s+conducted|systematic\\s+search(?:es)?|\n",
    "        literature\\s+search(?:es)?(?:\\s+was|\\s+were)?(?:\\s+conducted|\\s+performed)?|\n",
    "        up\\s+to\\s+our\\s+last\\s+search\\s+on|Cochrane\\s+Database\\s+of\\s+Systematic\\s+Reviews\\s+up\\s+to))  # Keywords\n",
    "        [\\s\\S]*?  # Match across lines\n",
    "        (\\d{1,2}(?:-|\\s)?(?:st|nd|rd|th)?(?:\\s|-)?(?:January|February|March|April|May|June|July|August|September|October|November|December)[,]?\\s\\d{4}|  # Full textual dates\n",
    "        (?:January|February|March|April|May|June|July|August|September|October|November|December)(?:\\s|-)?\\d{1,2}(?:st|nd|rd|th)?,?\\s\\d{4}|  # Month-first formats\n",
    "        \\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|  # Numeric formats like 12/12/2018\n",
    "        \\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\b(?:\\s|-)?\\d{4})  # Month + Year\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply regex to find matches\n",
    "    matches = list(re.finditer(pattern, document, re.IGNORECASE | re.VERBOSE))\n",
    "\n",
    "    # Debugging step: Print raw matches\n",
    "    print(\"Raw Matches:\", [match.groups() for match in matches])\n",
    "\n",
    "    # Extract and clean matched dates\n",
    "    dates = []\n",
    "    for match in matches:\n",
    "        if match.group(1):  # Ensure the group exists\n",
    "            dates.append(match.group(1).strip())\n",
    "\n",
    "    # Debugging step: Print extracted dates\n",
    "    print(\"Extracted Dates (Raw):\", dates)\n",
    "\n",
    "    # Deduplicate and sort dates\n",
    "    return sorted(set(dates))\n",
    "\n",
    "\n",
    "# Test Case\n",
    "document = \"\"\"\n",
    "A search for publications was carried out in April 2014, in the National Center for Biotechnology Information Advances Science and Health - US National Library of Medicine - National Institutes of Health - PubMed electronic databases, with no restrictions regarding date and language of publication. Additionally, a search was performed in the LILACS and SciELO databases using the descriptor \"Papillomavirus Vaccines\", followed by a manual search for randomized controlled trials (RCTs). In the first stage of article selection, the Decs/Mesh health descriptor \"papillomavirus vaccines/adverse effects\" was used. The study design filter \"RCTs\" was added to the obtained results. Subsequently, the identified articles were analyzed by reading the titles and abstracts.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Run the function\n",
    "extracted_dates = extract_last_literature_search_dates(document)\n",
    "print(\"Final Extracted Dates:\", extracted_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Services.Factories.Sections.ArticleExtractorFactory import ArticleExtractorFactory\n",
    "\n",
    "# url_ncbi = 'https://pubmed.ncbi.nlm.nih.gov/35430833/'\n",
    "# ncbi_extractor = ArticleExtractorFactory.get_extractor(url_ncbi)\n",
    "# print(\"Available sections (NCBI):\", ncbi_extractor.get_available_sections())\n",
    "# print(\"Abstract (NCBI):\\n\", ncbi_extractor.get_abstract())\n",
    "\n",
    "\n",
    "# url_ncbi = 'https://pmc.ncbi.nlm.nih.gov/articles/PMC9768633/'\n",
    "# ncbi_extractor = ArticleExtractorFactory.get_extractor(url_ncbi)\n",
    "# print(\"Available sections (NCBI):\", ncbi_extractor.get_available_sections())\n",
    "# print(\"Abstract (NCBI):\\n\", ncbi_extractor.get_abstract())\n",
    "\n",
    "# url_bmj = 'https://bmjopen.bmj.com/content/8/4/e019206'\n",
    "# bmj_extractor = ArticleExtractorFactory.get_extractor(url_bmj)\n",
    "# print(\"Available sections (BMJ):\", bmj_extractor.get_available_sections())\n",
    "# print(\"Abstract (BMJ):\\n\", bmj_extractor.get_abstract())\n",
    "\n",
    "# url_journal = 'https://journal.waocp.org/article_88640_d6458e9820c0023169822ff1d1fc3b2e.pdf'\n",
    "# journal_extractor = ArticleExtractorFactory.get_extractor(url_journal)\n",
    "# print(\"Available sections (journal):\", journal_extractor.get_available_sections())\n",
    "# print(\"Abstract (journal):\\n\", journal_extractor.get_abstract())\n",
    "\n",
    "# url_cochrane = 'https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD013717.pub2/full'\n",
    "# cochrane_extractor = ArticleExtractorFactory.get_extractor(url_cochrane)\n",
    "# print(\"Available sections (Cochrane):\", cochrane_extractor.get_available_sections())\n",
    "# print(\"Abstract (Cochrane):\\n\", cochrane_extractor.get_abstract())\n",
    "\n",
    "\n",
    "url_tandfonline = 'https://www.nature.com/articles/s41598-021-83727-7'\n",
    "tandfonline_extractor = ArticleExtractorFactory.get_extractor(url_tandfonline)\n",
    "print(\"Available sections (tandfonline):\", tandfonline_extractor.get_available_sections())\n",
    "print(\"Abstract (tandfonline):\\n\", tandfonline_extractor.get_abstract())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tandfonline_extractor.get_section(\"Main Content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Utils.Helpers import process_prisma_images\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = \"https://www.mdpi.com/1660-4601/19/15/9425\"\n",
    "# url = 'https://pmc.ncbi.nlm.nih.gov/articles/PMC8021610/'\n",
    "# Example URL\n",
    "# url = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC8477621/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://google.com\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "extracted_text = process_prisma_images(soup, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DOIEnricher import DOIEnricher\n",
    "enricher = DOIEnricher(\"testing_data.csv\")\n",
    "enricher.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check if versions of different requirement files are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def parse_requirements(file_path):\n",
    "    \"\"\"Reads a requirements file and returns a dictionary of package: version.\"\"\"\n",
    "    requirements = {}\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith(\"#\"):  # Ignore empty lines and comments\n",
    "                    if \"==\" in line:  # Handle versioned dependencies\n",
    "                        package, version = line.split(\"==\")\n",
    "                        requirements[package.strip()] = version.strip()\n",
    "                    else:\n",
    "                        requirements[line.strip()] = \"Unknown\"  # No version specified\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: File {file_path} not found.\")\n",
    "        sys.exit(1)\n",
    "    return requirements\n",
    "\n",
    "def compare_requirements(file1, file2):\n",
    "    \"\"\"Compares two requirements files and prints the differences.\"\"\"\n",
    "    req1 = parse_requirements(file1)\n",
    "    req2 = parse_requirements(file2)\n",
    "\n",
    "    added = req2.keys() - req1.keys()\n",
    "    removed = req1.keys() - req2.keys()\n",
    "    changed = {pkg: (req1[pkg], req2[pkg]) for pkg in req1.keys() & req2.keys() if req1[pkg] != req2[pkg]}\n",
    "\n",
    "    print(\"\\n📌 Requirements Differences:\\n\")\n",
    "\n",
    "    if added:\n",
    "        print(\"✅ Added Packages:\")\n",
    "        for pkg in added:\n",
    "            print(f\"   + {pkg}=={req2[pkg]}\")\n",
    "    else:\n",
    "        print(\"✅ No new packages added.\")\n",
    "\n",
    "    if removed:\n",
    "        print(\"\\n❌ Removed Packages:\")\n",
    "        for pkg in removed:\n",
    "            print(f\"   - {pkg}=={req1[pkg]}\")\n",
    "    else:\n",
    "        print(\"\\n❌ No packages removed.\")\n",
    "\n",
    "    if changed:\n",
    "        print(\"\\n🔄 Updated Packages:\")\n",
    "        for pkg, versions in changed.items():\n",
    "            print(f\"   ~ {pkg}: {versions[0]} → {versions[1]}\")\n",
    "    else:\n",
    "        print(\"\\n🔄 No packages updated.\")\n",
    "\n",
    "# Usage: python compare_requirements.py old_requirements.txt new_requirements.txt\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    compare_requirements(\"requirements.txt\", \"requirements2.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
